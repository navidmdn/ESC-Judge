from tqdm import tqdm
from fire import Fire
import pickle
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

evaluation_prompt = """I need to evaluate human preference of an AI assistant through a conversation between an AI assistant and a user. As a data
annotator, please help me rate the score according to the following rules:
The human preference mainly evaluate the degree of human preference towards the responses generated by an AI assistant. After
reading the dialogues, please envision yourself as a stressed individual and score the following rules based on the content of
the conversation. And here are the rules details:
0: I do not like this AI assistant.
1: I do not have any particular feelings.
2: It's okay, I'll reconsider using it myself.
3: I will use it when I am stressed.
4: I will use it myself and recommend it to friends.

[Input]
Dialogue between user and AI assistant:

{Dialogue}

Based on the rules, give your emotional knowledge score (The number only) to the Dialogue.
"""

def score_conversations(evaluation_data_path='output/ESC-RANK-style-chat-with-metadata.pkl',
                        evaluator_model_name='./ESC-RANK', output_path='output/ESC-RANK-results.pkl',
                        cache_dir=None):

    # print(evaluation_prompt)
    #todo: assuming data is formatted in pickle (there is a section in NB)
    with open(evaluation_data_path, 'rb') as f:
        eval_data = pickle.load(f)

    print(eval_data[0])

    tokenizer = AutoTokenizer.from_pretrained(evaluator_model_name,
                                              trust_remote_code=True, cache_dir=cache_dir)
    model = AutoModelForCausalLM.from_pretrained(evaluator_model_name,
                                                 torch_dtype=torch.float16, cache_dir=cache_dir,
                                                 trust_remote_code=True).cuda()
    result = []

    for dialog_obj in tqdm(eval_data):
        dialog = dialog_obj['chat']
        chat_string = ""
        eval_obj = dialog_obj.copy()

        for i, turn in enumerate(dialog):
            speaker = 'Assistant' if i%2 == 0 else 'User'
            chat_string += f"{speaker}: {turn}\n\n"


        model = model.eval()
        prompt = evaluation_prompt.format(Dialogue=chat_string)
        response, history = model.chat(tokenizer, prompt, history=[])

        print("----------"*5)
        print(response)
        print("----------"*5)

        eval_obj['ESC-RANK'] = response
        result.append(eval_obj)

    with open(output_path, 'wb') as f:
        pickle.dump(result, f)


if __name__ == '__main__':
    Fire(score_conversations)